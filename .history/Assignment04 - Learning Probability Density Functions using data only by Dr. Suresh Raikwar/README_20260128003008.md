# Learning Probability Density Functions using GANs

## Overview

This project implements a **Generative Adversarial Network (GAN)** to learn the unknown probability density function (PDF) of a transformed environmental dataset. By using NO₂ concentration data from the India Air Quality dataset, we demonstrate that a GAN can implicitly model complex, non-linear distributions without assuming a parametric form (like Gaussian or Exponential).

---

## Methodology

### 1. Data Preprocessing & Transformation

The raw feature **X** (NO₂ concentration) is subjected to a non-linear transformation based on university roll number parameters to create a complex target distribution **Z**.

* **Normalization:** **X** is scaled to a range of **[0, 5]** to ensure numerical stability.
* **Transformation Function:** 
  
  **Z = X_norm + A · sin(B · X_norm)**

*For Roll Number 102313005, **A = 2.50** and **B = 0.60**.*

### 2. GAN Framework

The core task is solved using two competing neural networks:

* **The Generator (G):** A 3-layer MLP that transforms random noise **ε ~ N(0,1)** into the **Z** space.
* **The Discriminator (D):** A binary classifier that learns to distinguish between real samples (**Z_real**) and fake samples (**Z_fake**) produced by the generator.

#### Generator Architecture
```
Input (1) → Linear(128) → LeakyReLU → Linear(256) → LeakyReLU → Linear(1) → Output
```

#### Discriminator Architecture
```
Input (1) → Linear(256) → LeakyReLU → Linear(128) → LeakyReLU → Linear(1) → Sigmoid → Output
```

### 3. Training Procedure

* **Loss Function:** Binary Cross Entropy (BCE) Loss
* **Optimization:** Adam Optimizer with learning rate of **1×10⁻⁴**
* **Beta Parameters:** (0.5, 0.9)
* **Iterations:** 15,000 epochs with a batch size of 64

---

## Results

### Result Table: Training Progress

The following table highlights the stability of the adversarial training process. A Discriminator loss near **1.386** indicates that the generator has successfully "fooled" the discriminator to the point of 50/50 uncertainty.

| Epoch | Discriminator Loss | Generator Loss | Status |
|-------|-------------------|----------------|--------|
| 0 | 1.386 | 0.676 | Initialization |
| 3000 | 1.392 | 0.694 | Stable Competition |
| 6000 | 1.384 | 0.688 | Balanced Training |
| 9000 | 1.377 | 0.702 | Converging |
| 12000 | 1.391 | 0.685 | Near Convergence |
| 15000 | 1.397 | 0.689 | Convergence Reached |

### Result Graph: PDF Approximation

The quality of the learned distribution is evaluated using **Kernel Density Estimation (KDE)** to compare the real transformed samples against the generator's output.

**Visual Comparison:**
- **Blue (Actual Z):** The true density derived from the transformed NO₂ data
- **Red Dashed (GAN Z):** The density of samples generated by the trained GAN

---

## Observations

1. **Mode Coverage:** The Generator successfully captured the broad distribution shifts created by the sine transformation. With **A = 2.50** and **B = 0.60**, the GAN effectively mapped the density over the entire range without collapsing to a single mean value.

2. **Training Stability:** The use of **LeakyReLU** activations and a specific learning rate (**1×10⁻⁴**) prevented the "Vanishing Gradient" problem commonly seen in GANs. The loss remained consistent throughout the final 5,000 iterations.

3. **Distribution Quality:** The overlap between the blue (Real) and red (GAN) KDE curves indicates that the network learned the implicit PDF of the data accurately without any prior knowledge of the underlying mathematical formula.

4. **Nash Equilibrium:** The discriminator and generator losses converged to a stable equilibrium around **~1.39** and **~0.69** respectively, indicating successful adversarial training.

---

## Technical Details

### Dependencies
```python
pandas
numpy
torch
matplotlib
seaborn
```

### Key Parameters
- **UID:** 102313005
- **A_VAL:** 2.50 (calculated as 0.5 × (UID % 7))
- **B_VAL:** 0.60 (calculated as 0.3 × (UID % 5 + 1))
- **Training Iterations:** 15,000
- **Batch Size:** 64
- **Learning Rate:** 0.0001
- **Optimizer:** Adam (β₁=0.5, β₂=0.9)

---

## How to Run

1. **Upload Dataset:** Place `data.csv` in the `/content/` directory (for Google Colab) or update the `SRC` path accordingly.

2. **Install Dependencies:**
   ```bash
   pip install pandas numpy torch matplotlib seaborn
   ```

3. **Execute the Notebook:**
   - Open `ASSIGNMENT_4.ipynb`
   - Run all cells sequentially
   - The parameters (A, B) will be automatically calculated based on the hardcoded UID

4. **View Results:**
   - Training progress will be printed every 3000 epochs
   - Final KDE plot will display the comparison between actual and generated distributions

---

## File Structure

```
Assignment04 - Learning Probability Density Functions using data only by Dr. Suresh Raikwar/
│
├── ASSIGNMENT_4.ipynb    # Main implementation notebook
├── README.md             # This file
└── data.csv              # India Air Quality dataset (not included)
```

---

## Conclusion

This project demonstrates the power of **Generative Adversarial Networks** in learning complex probability distributions without parametric assumptions. The GAN successfully approximated the non-linear transformation of environmental data, showcasing its potential for:

- Non-parametric density estimation
- Data augmentation for rare events
- Understanding complex environmental patterns

The results confirm that deep learning can be effectively applied to statistical modeling tasks traditionally handled by classical methods.

---

## Author

**University Roll Number:** 102313005  
**Course:** Data Science  
**Instructor:** Dr. Suresh Raikwar

---

## References

1. Goodfellow, I., et al. (2014). "Generative Adversarial Networks"
2. India Air Quality Dataset - NO₂ Concentration Measurements
3. PyTorch Documentation - Neural Network Modules

---

*This project was completed as part of Assignment 04 for learning probability density functions using data-driven approaches.*
