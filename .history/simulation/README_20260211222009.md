# Predicting CartPole Performance with Regression Models

## Project Summary

This experimental study explores whether machine learning can predict how long a CartPole simulation will last based on where it ends up. Using the CartPole-v1 simulation from Gymnasium, I generated performance data and tested six different regression algorithms to see which one could best estimate episode duration from the system's final configuration.

## How This Works

### Setting Up the Simulation

I worked with CartPole-v1, a well-known balance control simulation. Think of it as trying to keep a broomstick balanced on your hand while moving around - the cart slides left and right on a track, trying to keep a pole from falling over. The system tracks four key measurements:

- **Cart Position**: Where the cart is along the track
- **Cart Velocity**: How fast it's moving
- **Pole Angle**: How tilted the pole is from standing straight
- **Pole Angular Velocity**: How quickly the pole is tipping

### Creating the Dataset

I ran the simulation 1,000 times with completely random movements to build my dataset:

```
Total Runs: 1000
Control Method: Random actions
Features Collected: 4 state measurements
Goal: Predict total survival time
```

What happens in each run:
1. Reset everything to starting position
2. Make random left/right movements until the pole falls or cart goes off-track
3. Save the four measurements from when it ended
4. Count how many time steps it survived (this becomes our prediction target)

**Sample Data:**

| cart_position | cart_velocity | pole_angle | pole_angular_velocity | episode_reward |
| ------------- | ------------- | ---------- | --------------------- | -------------- |
| 0.062090      | 0.951053      | -0.230231  | -1.783494             | 11.0           |
| -0.153119     | -0.632762     | 0.218400   | 1.118507              | 13.0           |
| 0.145367      | 1.189654      | -0.244695  | -2.130400             | 18.0           |
| 0.170634      | 0.358668      | -0.229332  | -0.857255             | 12.0           |
| 0.190781      | 1.403324      | -0.230899  | -2.408421             | 25.0           |

### Preparing the Data

**Splitting Strategy:**
- 80% for training the models (800 examples)
- 20% held back for testing (200 examples)
- Used seed 42 to ensure consistent results across runs

**Normalizing Features:**
- Standardized all input values using StandardScaler
- Brings everything to the same scale (mean of 0, standard deviation of 1)
- Ensures no single measurement dominates just because of its numerical range

### Testing Different Algorithms

I experimented with six regression approaches to compare their prediction abilities:

1. **Linear Regression**: Simple straight-line fitting approach
2. **Ridge Regression**: Similar to linear but with penalty terms to reduce overfitting
3. **K-Nearest Neighbors**: Makes predictions by looking at the k most similar training examples
4. **Decision Tree**: Builds a flowchart-like structure of decision rules
5. **Random Forest**: Combines predictions from multiple decision trees
6. **Gradient Boosting**: Builds trees sequentially, each one correcting mistakes from the previous

### Performance Metrics

I measured success using three different standards:

- **MAE (Mean Absolute Error)**: Average prediction error in absolute terms
- **RMSE (Root Mean Squared Error)**: Similar to MAE but punishes bigger mistakes harder
- **R² Score**: How much of the variation the model explains (1.0 means perfect predictions, 0.0 means no better than guessing the average)

## What I Discovered

### Comparing All Six Models

Here's how each algorithm performed on the test set, ranked from best to worst by RMSE:

| Model             | MAE   | RMSE   | R² Score |
| ----------------- | ----- | ------ | -------- |
| KNN               | 6.633 | 9.076  | 0.417    |
| Random Forest     | 6.982 | 9.166  | 0.405    |
| Gradient Boosting | 7.475 | 9.773  | 0.324    |
| Decision Tree     | 8.735 | 11.349 | 0.088    |
| Linear Regression | 9.232 | 11.690 | 0.033    |
| Ridge Regression  | 9.243 | 11.708 | 0.030    |

### Key Findings

1. **Best Model: K-Nearest Neighbors (KNN)**
   - Achieved the lowest RMSE of 9.076
   - Highest R² score of 0.417, explaining 41.7% of variance
   - MAE of 6.633 indicates predictions are off by ~6-7 steps on average

2. **Second Best: Random Forest**
   - Very close performance to KNN (RMSE: 9.166)
   - R² score of 0.405
   - More robust and less prone to overfitting than single trees

3. **Poor Performance: Linear Models**
   - Linear and Ridge regression performed worst
   - R² scores near 0.03 indicate poor fit
   - Suggests non-linear relationship between state variables and rewards

4. **Ensemble Methods Outperform**
   - Random Forest and Gradient Boosting (ensemble methods) outperformed Decision Tree
   - Demonstrates the power of combining multiple models

### Visualization: Random Forest Predictions

![Actual vs Predicted Episode Reward](download.png)

**Graph Analysis:**

The scatter plot shows the relationship between actual and predicted episode rewards using the Random Forest model:

- **X-axis**: Actual episode rewards (ground truth)
- **Y-axis**: Predicted episode rewards (model output)
- **Red dashed line**: Perfect prediction line (y = x)
- **Teal points**: Individual test samples

**Observations:**

- Points scattered around the perfect prediction line indicate reasonable performance
- Clustering of points in the 10-30 reward range (most common episode lengths)
- Some spread shows the model struggles with high reward episodes (50+)
- The model tends to underpredict high-performing episodes
- Lower variance in predictions compared to actual values

## Interpretation

The moderate R² scores (max 0.417) suggest that final state variables alone provide limited information about total episode rewards. This makes sense because:

1. **Random Actions**: Episodes used random actions, creating high variability
2. **Final State Focus**: Only final states were recorded, losing trajectory information
3. **Stochastic Nature**: Same final state can result from different episode lengths
4. **Missing Information**: Action history and intermediate states matter significantly

Despite these limitations, KNN and Random Forest captured meaningful patterns, demonstrating that final state variables do contain predictive information about episode performance.

## Dependencies

```
gymnasium[classic_control]
numpy
pandas
matplotlib
scikit-learn
```

Install with:

```bash
pip install gymnasium[classic_control] numpy pandas matplotlib scikit-learn
```

## Usage

Run the simulation:

```bash
python cartpole_regression.py
```

The script will:

1. Generate 1000 episodes of CartPole data
2. Train 6 regression models
3. Display model comparison table
4. Show visualization of Random Forest predictions

## Conclusion

This analysis demonstrates that machine learning regression models can predict CartPole episode rewards with moderate accuracy based on final state variables. KNN emerged as the best performer, suggesting that similar final states tend to produce similar episode lengths. The project highlights the importance of model selection and the value of ensemble methods in handling complex, non-linear relationships in reinforcement learning environments.
