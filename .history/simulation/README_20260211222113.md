# Predicting CartPole Performance with Regression Models

**Author:** Angad Singh Madhok  
**Roll Number:** 10231005  
**Branch:** COE

## Project Summary

This experimental study explores whether machine learning can predict how long a CartPole simulation will last based on where it ends up. Using the CartPole-v1 simulation from Gymnasium, I generated performance data and tested six different regression algorithms to see which one could best estimate episode duration from the system's final configuration.

## How This Works

### Setting Up the Simulation

I worked with CartPole-v1, a well-known balance control simulation. Think of it as trying to keep a broomstick balanced on your hand while moving around - the cart slides left and right on a track, trying to keep a pole from falling over. The system tracks four key measurements:

- **Cart Position**: Where the cart is along the track
- **Cart Velocity**: How fast it's moving
- **Pole Angle**: How tilted the pole is from standing straight
- **Pole Angular Velocity**: How quickly the pole is tipping

### Creating the Dataset

I ran the simulation 1,000 times with completely random movements to build my dataset:

```
Total Runs: 1000
Control Method: Random actions
Features Collected: 4 state measurements
Goal: Predict total survival time
```

What happens in each run:
1. Reset everything to starting position
2. Make random left/right movements until the pole falls or cart goes off-track
3. Save the four measurements from when it ended
4. Count how many time steps it survived (this becomes our prediction target)

**Sample Data:**

| cart_position | cart_velocity | pole_angle | pole_angular_velocity | episode_reward |
| ------------- | ------------- | ---------- | --------------------- | -------------- |
| 0.062090      | 0.951053      | -0.230231  | -1.783494             | 11.0           |
| -0.153119     | -0.632762     | 0.218400   | 1.118507              | 13.0           |
| 0.145367      | 1.189654      | -0.244695  | -2.130400             | 18.0           |
| 0.170634      | 0.358668      | -0.229332  | -0.857255             | 12.0           |
| 0.190781      | 1.403324      | -0.230899  | -2.408421             | 25.0           |

### Preparing the Data

**Splitting Strategy:**
- 80% for training the models (800 examples)
- 20% held back for testing (200 examples)
- Used seed 42 to ensure consistent results across runs

**Normalizing Features:**
- Standardized all input values using StandardScaler
- Brings everything to the same scale (mean of 0, standard deviation of 1)
- Ensures no single measurement dominates just because of its numerical range

### Testing Different Algorithms

I experimented with six regression approaches to compare their prediction abilities:

1. **Linear Regression**: Simple straight-line fitting approach
2. **Ridge Regression**: Similar to linear but with penalty terms to reduce overfitting
3. **K-Nearest Neighbors**: Makes predictions by looking at the k most similar training examples
4. **Decision Tree**: Builds a flowchart-like structure of decision rules
5. **Random Forest**: Combines predictions from multiple decision trees
6. **Gradient Boosting**: Builds trees sequentially, each one correcting mistakes from the previous

### Performance Metrics

I measured success using three different standards:

- **MAE (Mean Absolute Error)**: Average prediction error in absolute terms
- **RMSE (Root Mean Squared Error)**: Similar to MAE but punishes bigger mistakes harder
- **R² Score**: How much of the variation the model explains (1.0 means perfect predictions, 0.0 means no better than guessing the average)

## What I Discovered

### Comparing All Six Models

Here's how each algorithm performed on the test set, ranked from best to worst by RMSE:

| Model             | MAE   | RMSE   | R² Score |
| ----------------- | ----- | ------ | -------- |
| KNN               | 6.633 | 9.076  | 0.417    |
| Random Forest     | 6.982 | 9.166  | 0.405    |
| Gradient Boosting | 7.475 | 9.773  | 0.324    |
| Decision Tree     | 8.735 | 11.349 | 0.088    |
| Linear Regression | 9.232 | 11.690 | 0.033    |
| Ridge Regression  | 9.243 | 11.708 | 0.030    |

### Main Takeaways

1. **KNN Came Out on Top**
   - Lowest error rate with RMSE of 9.076
   - Best explanatory power at 41.7% (R² = 0.417)
   - Typically off by about 6-7 steps when predicting episode length

2. **Random Forest Was a Close Second**
   - Nearly matched KNN with RMSE of 9.166
   - Captured 40.5% of the variance
   - Generally more stable and reliable than single decision trees

3. **Linear Approaches Struggled**
   - Both linear methods showed R² values around 0.03
   - This reveals the relationship isn't simply linear
   - Final state measurements connect to episode length in complex, non-linear ways

4. **Tree Combinations Beat Single Trees**
   - Both Random Forest and Gradient Boosting significantly outperformed the standalone Decision Tree
   - Shows the benefit of aggregating multiple models

### Visual Analysis: Random Forest Results

![Actual vs Predicted Episode Reward](download.png)

**Understanding the Graph:**

This scatter plot compares what actually happened versus what the Random Forest model predicted:

- **Horizontal axis**: Real survival times from test episodes
- **Vertical axis**: What the model guessed for those same episodes
- **Red diagonal line**: Where points would fall if predictions were perfect
- **Teal dots**: Each represents one test case

**What the Pattern Tells Us:**
- Most dots cluster near the diagonal, showing decent accuracy
- Heavy concentration between 10-30 steps (typical episode duration)
- Wider scatter at higher values means the model has trouble with longer episodes
- The model tends to guess lower than reality for episodes that lasted a long time
- Predictions show less variation than actual results

## Making Sense of the Results

The R² scores topping out at 0.417 tell us that knowing just the final measurements gives us limited prediction power. Several factors explain this:

1. **Randomness Built In**: Since movements were random, there's inherent unpredictability
2. **Snapshot Limitation**: We only looked at where things ended, not how they got there
3. **Path Dependency**: Different sequences can lead to similar final positions
4. **Lost Context**: What happened during the episode matters but wasn't captured

Even with these constraints, both KNN and Random Forest found useful patterns. This proves that the ending configuration does reveal something meaningful about overall performance, even if it's not the complete picture.

## Required Libraries

```
gymnasium[classic_control]
numpy
pandas
matplotlib
scikit-learn
```

Installation command:
```bash
pip install gymnasium[classic_control] numpy pandas matplotlib scikit-learn
```

## Running the Code

Execute the main script:
```bash
python cartpole_regression.py
```

This will:
1. Simulate 1000 CartPole episodes with random actions
2. Train all six regression models
3. Print performance comparison table
4. Display Random Forest prediction visualization

## Final Thoughts

This experiment shows that we can achieve moderate success predicting CartPole episode duration from ending state measurements alone. KNN delivered the strongest results, which suggests that episodes ending in similar configurations tend to have comparable lengths. The clear advantage of KNN and Random Forest over simpler linear methods underscores how non-linear patterns dominate this problem. It's a good reminder that choosing the right algorithm matters, and that combining multiple models (ensemble techniques) often handles complexity better than single approaches.
